#!/usr/bin/env python3
"""
Gemini Oracle - AI Orchestration System for Claude Code

This CLI tool consults Gemini 3 Pro as an oracle for architectural decisions
and development guidance. It reads project context from FULLAUTO_CONTEXT.md
and returns structured JSON responses.

Features:
- Retains last 5 conversation exchanges per project
- Structured JSON responses with steps, risks, success criteria
- File attachment with line range support
- Image input for visual analysis
- Image generation with 'imagine' command
- Token counting and warnings
- Debug logging for troubleshooting

Usage:
    oracle ask "How should I implement feature X?"
    oracle ask --files src/main.py "Validate this"
    oracle ask --image screenshot.png "What's wrong with this UI?"
    oracle imagine "Logo for a fintech startup" --output logo.png
    oracle history                       # View conversation history
"""

import argparse
import base64
import hashlib
import json
import mimetypes
import os
import subprocess
import sys
import logging
import re
import tempfile
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Tuple

try:
    import requests
except ImportError:
    requests = None  # Will error if Vast.ai auto-provision is needed

# OAuth imports for Google account login
try:
    from google.auth.transport.requests import Request as AuthRequest
    from google.oauth2.credentials import Credentials
    from google_auth_oauthlib.flow import InstalledAppFlow
    OAUTH_AVAILABLE = True
except ImportError:
    OAUTH_AVAILABLE = False

# Setup logging
LOG_DIR = Path.home() / ".oracle" / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)
LOG_FILE = LOG_DIR / f"oracle_{datetime.now().strftime('%Y%m%d')}.log"

# History storage
HISTORY_DIR = Path.home() / ".oracle" / "history"
HISTORY_DIR.mkdir(parents=True, exist_ok=True)
MAX_HISTORY_EXCHANGES = 5

# Image output directory
IMAGE_OUTPUT_DIR = Path.home() / ".oracle" / "images"
IMAGE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Token limits
CHARS_PER_TOKEN = 4  # Rough estimate for code
TOKEN_WARNING_THRESHOLD = 200000  # Warn when approaching this
FILE_TOKEN_WARNING = 15000  # Warn if single file exceeds this

# Supported image formats
SUPPORTED_IMAGE_FORMATS = {'.png', '.jpg', '.jpeg', '.gif', '.webp', '.bmp'}

# Vast.ai remote server config for geo-restricted features
# Can be overridden via environment variables:
#   ORACLE_VASTAI_HOST, ORACLE_VASTAI_PORT, ORACLE_VASTAI_USER
VASTAI_CONFIG = {
    'host': os.environ.get('ORACLE_VASTAI_HOST', 'ssh7.vast.ai'),
    'port': int(os.environ.get('ORACLE_VASTAI_PORT', '15490')),
    'user': os.environ.get('ORACLE_VASTAI_USER', 'root'),
    'timeout': 120,  # seconds
}

# Vast.ai API config for auto-provisioning
VASTAI_API_BASE = "https://console.vast.ai/api/v0"
VASTAI_AUTO_PROVISION = {
    'enabled': True,
    'docker_image': 'pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime',
    'disk_gb': 10,
    'max_price_per_hour': 0.50,  # Max $/hr willing to pay
    'startup_timeout': 300,  # 5 minutes max to wait for instance
    'ssh_timeout': 60,  # 1 minute max to wait for SSH
}

# OAuth configuration for Google account login
OAUTH_DIR = Path.home() / ".oracle" / "oauth"
OAUTH_DIR.mkdir(parents=True, exist_ok=True)
OAUTH_TOKEN_FILE = OAUTH_DIR / "token.json"
OAUTH_CLIENT_SECRET_FILE = OAUTH_DIR / "client_secret.json"
OAUTH_SCOPES = [
    'https://www.googleapis.com/auth/cloud-platform',
]

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE),
    ]
)
logger = logging.getLogger("oracle")

# Console handler for debug mode (added dynamically)
console_handler = logging.StreamHandler(sys.stderr)
console_handler.setFormatter(logging.Formatter('üîÆ %(levelname)s: %(message)s'))


def log_debug(msg: str, debug_mode: bool = False):
    """Log to file always, to console if debug mode enabled."""
    logger.debug(msg)
    if debug_mode:
        console_handler.setLevel(logging.DEBUG)
        if console_handler not in logger.handlers:
            logger.addHandler(console_handler)
        print(f"üîÆ DEBUG: {msg}", file=sys.stderr)


def log_error(msg: str):
    """Log error to file and console."""
    logger.error(msg)
    print(f"üîÆ ERROR: {msg}", file=sys.stderr)


def log_info(msg: str, debug_mode: bool = False):
    """Log info."""
    logger.info(msg)
    if debug_mode:
        print(f"üîÆ INFO: {msg}", file=sys.stderr)


def log_warning(msg: str):
    """Log warning to file and console."""
    logger.warning(msg)
    print(f"üîÆ WARNING: {msg}", file=sys.stderr)


def estimate_tokens(text: str) -> int:
    """Estimate token count from text (rough approximation)."""
    return len(text) // CHARS_PER_TOKEN


def check_fullauto_context_health():
    """
    Check that FULLAUTO_CONTEXT.md is NOT gitignored and IS tracked.
    This is critical for conversation continuity during compaction.
    Auto-fixes .gitignore if the file is incorrectly ignored.
    """
    context_file = Path.cwd() / "FULLAUTO_CONTEXT.md"
    gitignore_file = Path.cwd() / ".gitignore"

    if not context_file.exists():
        return  # No context file yet, nothing to check

    # Check if in a git repo
    git_dir = Path.cwd() / ".git"
    if not git_dir.exists():
        return  # Not a git repo

    # Check if gitignored
    try:
        result = subprocess.run(
            ["git", "check-ignore", "-q", "FULLAUTO_CONTEXT.md"],
            capture_output=True,
            cwd=Path.cwd()
        )
        is_gitignored = result.returncode == 0
    except Exception:
        return  # Git not available

    if is_gitignored:
        print("\n‚ö†Ô∏è  WARNING: FULLAUTO_CONTEXT.md is in .gitignore!")
        print("   This file MUST be tracked for conversation continuity.")
        print("   Attempting to fix .gitignore...\n")

        # Try to remove it from .gitignore and add a warning comment
        if gitignore_file.exists():
            content = gitignore_file.read_text()
            lines = content.split('\n')
            new_lines = []
            found_and_removed = False

            for line in lines:
                stripped = line.strip()
                # Skip lines that ignore FULLAUTO_CONTEXT.md
                if stripped == "FULLAUTO_CONTEXT.md" or stripped == "/FULLAUTO_CONTEXT.md":
                    found_and_removed = True
                    # Add warning comment instead
                    new_lines.append("# FULLAUTO_CONTEXT.md - DO NOT IGNORE THIS FILE!")
                    new_lines.append("# It is required for conversation continuity during compaction.")
                    new_lines.append("# See: https://github.com/n1ira/claude-oracle")
                else:
                    new_lines.append(line)

            if found_and_removed:
                gitignore_file.write_text('\n'.join(new_lines))
                print("   ‚úì Fixed .gitignore - FULLAUTO_CONTEXT.md is no longer ignored")
                print("   Run: git add FULLAUTO_CONTEXT.md .gitignore && git commit -m 'Track FULLAUTO_CONTEXT.md'\n")
            else:
                print("   Could not auto-fix. Please manually remove FULLAUTO_CONTEXT.md from .gitignore\n")

    # Check if tracked in git
    try:
        result = subprocess.run(
            ["git", "ls-files", "FULLAUTO_CONTEXT.md"],
            capture_output=True,
            text=True,
            cwd=Path.cwd()
        )
        is_tracked = bool(result.stdout.strip())
    except Exception:
        return

    if not is_tracked and not is_gitignored:
        print("\n‚ö†Ô∏è  WARNING: FULLAUTO_CONTEXT.md is not tracked in git!")
        print("   Run: git add FULLAUTO_CONTEXT.md && git commit -m 'Track FULLAUTO_CONTEXT.md'\n")


def ensure_fullauto_header():
    """Check if FULLAUTO_CONTEXT.md exists and ensure full /fullauto command is at the bottom."""
    context_file = Path.cwd() / "FULLAUTO_CONTEXT.md"

    if not context_file.exists():
        return  # No context file, nothing to do

    content = context_file.read_text()

    # Read the /fullauto command file
    fullauto_command_path = Path.home() / ".claude" / "commands" / "fullauto.md"
    if not fullauto_command_path.exists():
        logger.warning(f"Could not find fullauto command at {fullauto_command_path}")
        return

    fullauto_content = fullauto_command_path.read_text()

    # Marker to identify where fullauto content starts
    separator = "\n\n---\n\n"
    disclaimer_marker = "<!-- AUTO-GENERATED FULLAUTO INSTRUCTIONS -->"
    fullauto_marker = "# FULLAUTO MODE - Gemini Oracle Orchestrated Development"

    # If fullauto command already exists in the file, remove it
    # Look for either the disclaimer or the fullauto marker
    for marker in [disclaimer_marker, fullauto_marker]:
        if marker in content:
            marker_index = content.find(marker)
            if marker_index != -1:
                # Look for the separator before the marker
                separator_before_marker = content.rfind(separator[:10], 0, marker_index)
                if separator_before_marker != -1:
                    content = content[:separator_before_marker].rstrip()
                else:
                    # If no separator found, just remove from marker onwards
                    content = content[:marker_index].rstrip()
                break

    # Disclaimer section
    disclaimer = f"""{disclaimer_marker}

# ‚ö†Ô∏è DO NOT EDIT BELOW THIS LINE ‚ö†Ô∏è

**The section below is auto-generated by the Oracle and contains critical instructions for FULLAUTO MODE.**

If you are Claude operating in fullauto mode, you MUST read and follow the instructions below to the letter. These instructions are essential for proper operation after conversation compaction.

**User:** You may edit the content above this line, but do NOT modify or remove anything below. The Oracle will automatically refresh this section to ensure Claude has the latest instructions.

---

"""

    # Always append the disclaimer and fullauto command to the bottom
    new_content = content + separator + disclaimer + fullauto_content
    context_file.write_text(new_content)
    logger.info("Ensured /fullauto command is at bottom of FULLAUTO_CONTEXT.md")


def get_gemini_api_key() -> Optional[str]:
    """Get Gemini API key from environment, checking multiple possible names."""
    return os.environ.get("GEMINI_API_KEY") or os.environ.get("GOOGLE_API_KEY")


def get_oauth_credentials(debug: bool = False) -> Optional['Credentials']:
    """
    Get OAuth credentials for Google account login.

    Returns cached credentials if valid, refreshes if expired, or None if not logged in.
    Use `oracle login` to initiate the OAuth flow.
    """
    if not OAUTH_AVAILABLE:
        log_debug("OAuth libraries not available", debug)
        return None

    if not OAUTH_TOKEN_FILE.exists():
        log_debug("No OAuth token file found", debug)
        return None

    try:
        creds = Credentials.from_authorized_user_file(str(OAUTH_TOKEN_FILE), OAUTH_SCOPES)

        if creds and creds.valid:
            log_debug("Using valid cached OAuth credentials", debug)
            return creds

        if creds and creds.expired and creds.refresh_token:
            log_debug("Refreshing expired OAuth credentials", debug)
            creds.refresh(AuthRequest())
            # Save refreshed token
            with open(OAUTH_TOKEN_FILE, 'w') as token:
                token.write(creds.to_json())
            return creds

        log_debug("OAuth credentials invalid and cannot be refreshed", debug)
        return None

    except Exception as e:
        log_debug(f"Error loading OAuth credentials: {e}", debug)
        return None


def oauth_login(debug: bool = False) -> bool:
    """
    Initiate OAuth login flow to authenticate with Google account.

    Requires client_secret.json in ~/.oracle/oauth/ directory.
    Opens browser for user to authorize access.

    Returns True on success, False on failure.
    """
    if not OAUTH_AVAILABLE:
        print("‚ùå OAuth libraries not installed. Run:")
        print("   pip install google-auth-oauthlib google-auth-httplib2")
        return False

    if not OAUTH_CLIENT_SECRET_FILE.exists():
        print("‚ùå OAuth client secret not found.")
        print()
        print("To set up Google OAuth login:")
        print("1. Go to https://console.cloud.google.com/apis/credentials")
        print("2. Create OAuth 2.0 Client ID (Desktop application)")
        print("3. Download the JSON and save as:")
        print(f"   {OAUTH_CLIENT_SECRET_FILE}")
        print()
        print("Or continue using API key with GEMINI_API_KEY environment variable.")
        return False

    try:
        print("üîê Starting Google OAuth login...")
        print("   A browser window will open for authorization.")
        print()

        flow = InstalledAppFlow.from_client_secrets_file(
            str(OAUTH_CLIENT_SECRET_FILE),
            OAUTH_SCOPES
        )

        # Run local server for OAuth callback
        creds = flow.run_local_server(port=0)

        # Save credentials for future use
        with open(OAUTH_TOKEN_FILE, 'w') as token:
            token.write(creds.to_json())

        print("‚úÖ Successfully logged in!")
        print(f"   Token saved to: {OAUTH_TOKEN_FILE}")
        print()
        print("You can now use Oracle with your Google account subscription.")
        return True

    except Exception as e:
        print(f"‚ùå OAuth login failed: {e}")
        log_error(f"OAuth login failed: {e}")
        return False


def oauth_logout() -> bool:
    """Remove saved OAuth credentials."""
    if OAUTH_TOKEN_FILE.exists():
        OAUTH_TOKEN_FILE.unlink()
        print("‚úÖ Logged out. OAuth credentials removed.")
        return True
    else:
        print("‚ÑπÔ∏è  No OAuth credentials found.")
        return False


def get_genai_client(api_key: Optional[str] = None, debug: bool = False):
    """
    Create a Gemini client using available authentication methods.

    Priority:
    1. Vertex AI Express (VERTEX_API_KEY) - works with Gemini 3 Pro
    2. OAuth credentials (if logged in via `oracle login`)
    3. Regular API key (GEMINI_API_KEY)

    Returns (client, auth_method) tuple or (None, error_message).
    """
    from google import genai

    # Try Vertex AI Express first (works with Gemini 3 Pro)
    vertex_key = os.environ.get("VERTEX_API_KEY")
    if vertex_key:
        log_info("Using Vertex AI Express", debug)
        try:
            client = genai.Client(vertexai=True, api_key=vertex_key)
            return (client, "vertex_express")
        except Exception as e:
            log_debug(f"Vertex AI Express client creation failed: {e}", debug)
            # Fall through to other methods

    # Try OAuth second
    oauth_creds = get_oauth_credentials(debug)
    if oauth_creds:
        log_info("Using OAuth credentials (Google account)", debug)
        try:
            client = genai.Client(credentials=oauth_creds)
            return (client, "oauth")
        except Exception as e:
            log_debug(f"OAuth client creation failed: {e}", debug)
            # Fall through to API key

    # Fall back to regular API key
    if not api_key:
        api_key = get_gemini_api_key()

    if api_key:
        log_info("Using API key", debug)
        client = genai.Client(api_key=api_key)
        return (client, "api_key")

    return (None, "No authentication available. Set VERTEX_API_KEY, run 'oracle login', or set GEMINI_API_KEY.")


def read_image_as_base64(image_path: str, debug: bool = False) -> Tuple[Optional[bytes], Optional[str], Optional[str]]:
    """
    Read an image file and return base64 data with mime type.

    Args:
        image_path: Path to the image file
        debug: Enable debug output

    Returns:
        Tuple of (raw_bytes, mime_type, error_message)
    """
    path = Path(image_path)

    if not path.exists():
        return (None, None, f"Image file not found: {image_path}")

    suffix = path.suffix.lower()
    if suffix not in SUPPORTED_IMAGE_FORMATS:
        return (None, None, f"Unsupported image format: {suffix}. Supported: {SUPPORTED_IMAGE_FORMATS}")

    # Determine mime type
    mime_type = mimetypes.guess_type(str(path))[0]
    if not mime_type:
        mime_map = {
            '.png': 'image/png',
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.gif': 'image/gif',
            '.webp': 'image/webp',
            '.bmp': 'image/bmp'
        }
        mime_type = mime_map.get(suffix, 'image/png')

    try:
        with open(path, 'rb') as f:
            image_data = f.read()

        file_size = len(image_data)
        log_debug(f"Read image {image_path}: {file_size:,} bytes, {mime_type}", debug)

        return (image_data, mime_type, None)

    except Exception as e:
        return (None, None, f"Error reading image: {e}")


try:
    from google import genai
    from google.genai import types
    log_debug("google-genai imported successfully")
except ImportError as e:
    log_error(f"Failed to import google-genai: {e}")
    print("ERROR: google-genai not installed. Run: pip install google-genai", file=sys.stderr)
    sys.exit(1)

from schemas import ORACLE_RESPONSE_SCHEMA, VALIDATION_RESPONSE_SCHEMA


# Oracle system prompts
ORACLE_SYSTEM_PROMPT = """You are the Gemini Oracle - a supreme AI architect and technical lead.

Your role is to provide high-level architectural guidance and key decisions for software development projects. You work alongside Claude Code, an expert implementation AI. Your job is to be the "lead architect" - making strategic decisions while Claude handles the implementation details.

RESPONSE TYPES - Choose based on the query:

1. "strategic_advice" - Use when the user asks questions, wants analysis, or needs recommendations.
   - Provide your answer in the "answer" field with full explanation
   - No need for implementation steps - you're giving advice, not a to-do list
   - Example queries: "Would X cause problems?", "What's the optimal ratio?", "Should I use A or B?"

2. "implementation_plan" - Use when the user wants you to plan a task for Claude to execute.
   - Provide ordered "steps" for Claude to follow
   - Include "risks" and "success_criteria"
   - Example queries: "How should I implement X?", "Plan a refactor of Y", "Create a system for Z"

IMPORTANT: YOU ARE THE ORACLE. Do NOT create steps like "consult the Oracle" - you ARE the Oracle being consulted. Answer directly.

PRINCIPLES:
1. Think deeply before deciding. Consider multiple approaches.
2. Be decisive - give clear recommendations, not wishy-washy advice.
3. Consider the full context of the project (provided in FULLAUTO_CONTEXT.md).
4. For implementation_plan: Break complex tasks into clear, ordered steps.
5. Identify risks proactively and provide mitigations.
6. You have access to recent conversation history - use it to maintain continuity.

When responding:
- Be concise but thorough
- Prioritize practical solutions over theoretical perfection
- If you need more information, ask clarifying questions
- Reference previous discussions when relevant
- You can instruct Claude to search the Web for latest docs, bug fixes, etc.

You are the strategic brain. Claude is the implementation hands. Together you are unstoppable."""

VALIDATION_SYSTEM_PROMPT = """You are the Gemini Oracle in VALIDATION mode.

Audit Claude's implementation against the original plan with high standards:
1. Check if the ACTUAL problem is solved, not just the surface symptoms
2. Identify any gaps, edge cases, or missed requirements
3. Be specific about what's incomplete - vague "looks good" is not acceptable
4. If 90% done, focus on the missing 10%

You have access to recent conversation history - use it to understand what was planned.

Provide a clear verdict: APPROVED, NEEDS_WORK, or REJECTED with specific remediation steps."""

IMAGE_ANALYSIS_SYSTEM_PROMPT = """You are the Gemini Oracle analyzing a visual artifact.

When analyzing images, consider:
1. If it's a UI/screenshot: Identify usability issues, design problems, accessibility concerns
2. If it's a diagram: Evaluate architecture, identify missing components, suggest improvements
3. If it's an error: Diagnose the root cause and provide solutions
4. If it's a mockup: Provide implementation guidance

Be specific and actionable in your analysis."""


def parse_file_spec(file_spec: str) -> Tuple[str, List[Tuple[int, int]]]:
    """
    Parse a file specification with optional line ranges.

    Formats:
        "src/main.py" -> ("src/main.py", [])  # whole file
        "src/main.py:10-50" -> ("src/main.py", [(10, 50)])
        "src/main.py:1-50,100-110" -> ("src/main.py", [(1, 50), (100, 110)])

    Returns:
        Tuple of (filepath, list of (start, end) line ranges)
    """
    if ':' not in file_spec:
        return (file_spec, [])

    parts = file_spec.split(':', 1)
    filepath = parts[0]
    ranges_str = parts[1]

    ranges = []
    for range_part in ranges_str.split(','):
        range_part = range_part.strip()
        if '-' in range_part:
            start, end = range_part.split('-', 1)
            try:
                ranges.append((int(start), int(end)))
            except ValueError:
                log_error(f"Invalid line range: {range_part}")
                continue
        else:
            # Single line
            try:
                line_num = int(range_part)
                ranges.append((line_num, line_num))
            except ValueError:
                log_error(f"Invalid line number: {range_part}")
                continue

    return (filepath, ranges)


def read_file_with_ranges(filepath: str, ranges: List[Tuple[int, int]], debug: bool = False) -> Tuple[str, int, str]:
    """
    Read a file, optionally extracting specific line ranges.

    Args:
        filepath: Path to the file
        ranges: List of (start, end) line ranges, or empty for whole file
        debug: Enable debug output

    Returns:
        Tuple of (content, token_count, range_description)
    """
    path = Path(filepath)

    if not path.exists():
        return (f"ERROR: File not found: {filepath}", 0, "")

    try:
        all_lines = path.read_text().splitlines()
        total_lines = len(all_lines)

        if not ranges:
            # Whole file
            content = "\n".join(all_lines)
            range_desc = f"(lines 1-{total_lines})"
        else:
            # Extract specific ranges
            extracted_lines = []
            range_parts = []

            for idx, (start, end) in enumerate(ranges):
                # Convert to 0-indexed, clamp to valid range
                start_idx = max(0, start - 1)
                end_idx = min(total_lines, end)

                if start_idx < end_idx:
                    # Add range marker if not first range
                    if extracted_lines and idx > 0:
                        prev_end = ranges[idx-1][1]
                        extracted_lines.append(f"\n... [lines {prev_end+1}-{start-1} omitted] ...\n")

                    for i in range(start_idx, end_idx):
                        extracted_lines.append(f"{i+1:4d} | {all_lines[i]}")

                    range_parts.append(f"{start}-{end}")

            content = "\n".join(extracted_lines)
            range_desc = f"(lines {', '.join(range_parts)})"

        tokens = estimate_tokens(content)
        log_debug(f"Read {filepath}: {len(content)} chars, ~{tokens} tokens", debug)

        return (content, tokens, range_desc)

    except Exception as e:
        return (f"ERROR reading {filepath}: {e}", 0, "")


def format_files_for_prompt(file_specs: List[str], debug: bool = False) -> Tuple[str, int, List[str], List[str]]:
    """
    Read and format multiple files for inclusion in prompt.

    Args:
        file_specs: List of file specifications (with optional line ranges)
        debug: Enable debug output

    Returns:
        Tuple of (formatted_content, total_tokens, files_reviewed, warnings)
    """
    if not file_specs:
        return ("", 0, [], [])

    sections = []
    total_tokens = 0
    files_reviewed = []
    warnings = []

    sections.append("\n\n## ATTACHED FILES")
    sections.append("‚îÄ" * 50)

    for file_spec in file_specs:
        filepath, ranges = parse_file_spec(file_spec)
        content, tokens, range_desc = read_file_with_ranges(filepath, ranges, debug)

        # Check for large file warning
        if tokens >= FILE_TOKEN_WARNING:
            warning = f"‚ö†Ô∏è FILE TOO LARGE: {filepath} is ~{tokens:,} tokens ({tokens * CHARS_PER_TOKEN:,} chars). Consider sending only relevant sections using line ranges (e.g., {filepath}:1-100,200-250)"
            warnings.append(warning)
            log_warning(warning)

        # Detect file extension for syntax highlighting
        ext = Path(filepath).suffix.lstrip('.')
        lang = ext if ext else "text"

        # Format the file section
        sections.append(f"\n### {filepath} {range_desc}")
        sections.append(f"```{lang}")
        sections.append(content)
        sections.append("```")

        total_tokens += tokens
        files_reviewed.append(file_spec)

    sections.append("\n" + "‚îÄ" * 50)
    sections.append("## END OF ATTACHED FILES\n")

    formatted = "\n".join(sections)

    log_debug(f"Formatted {len(file_specs)} files: ~{total_tokens} tokens total", debug)

    return (formatted, total_tokens, files_reviewed, warnings)


def get_project_id(debug: bool = False) -> str:
    """Generate a unique ID for the current project based on its path."""
    # Find project root (where FULLAUTO_CONTEXT.md is, or current dir)
    current = Path.cwd()
    project_root = current

    for _ in range(10):
        if (current / "FULLAUTO_CONTEXT.md").exists():
            project_root = current
            break
        if (current / ".git").exists():
            project_root = current
            break
        parent = current.parent
        if parent == current:
            break
        current = parent

    # Create a short hash of the project path
    path_hash = hashlib.md5(str(project_root).encode()).hexdigest()[:12]
    project_name = project_root.name
    project_id = f"{project_name}_{path_hash}"

    log_debug(f"Project ID: {project_id} (from {project_root})", debug)
    return project_id


def get_history_file(project_id: str) -> Path:
    """Get the history file path for a project."""
    return HISTORY_DIR / f"{project_id}.json"


def load_history(project_id: str, debug: bool = False) -> List[Dict]:
    """Load conversation history for a project."""
    history_file = get_history_file(project_id)

    if not history_file.exists():
        log_debug(f"No history file found for project {project_id}", debug)
        return []

    try:
        with open(history_file, 'r') as f:
            history = json.load(f)
        log_debug(f"Loaded {len(history)} exchanges from history", debug)
        return history
    except Exception as e:
        log_error(f"Failed to load history: {e}")
        return []


def save_history(project_id: str, history: List[Dict], debug: bool = False):
    """Save conversation history for a project (keeps last MAX_HISTORY_EXCHANGES)."""
    history_file = get_history_file(project_id)

    # Keep only the last N exchanges
    trimmed_history = history[-MAX_HISTORY_EXCHANGES:]

    try:
        with open(history_file, 'w') as f:
            json.dump(trimmed_history, f, indent=2)
        log_debug(f"Saved {len(trimmed_history)} exchanges to history", debug)
    except Exception as e:
        log_error(f"Failed to save history: {e}")


def clear_history(project_id: str, debug: bool = False) -> bool:
    """Clear conversation history for a project."""
    history_file = get_history_file(project_id)

    if history_file.exists():
        try:
            history_file.unlink()
            log_info(f"Cleared history for project {project_id}", debug)
            return True
        except Exception as e:
            log_error(f"Failed to clear history: {e}")
            return False
    return True


def format_history_for_prompt(history: List[Dict], debug: bool = False) -> Tuple[str, int]:
    """Format conversation history for inclusion in prompt."""
    if not history:
        return ("", 0)

    # Build history section with indicator
    history_lines = []
    history_lines.append("\n\n## CONVERSATION HISTORY")
    history_lines.append("‚îÄ" * 50)

    # Add indicator if there was older context
    total_exchanges = len(history)
    if total_exchanges >= MAX_HISTORY_EXCHANGES:
        history_lines.append(f"‚ö†Ô∏è  [Older context removed for efficiency - showing last {MAX_HISTORY_EXCHANGES} exchanges]")
        history_lines.append("‚îÄ" * 50)

    for i, exchange in enumerate(history, 1):
        timestamp = exchange.get('timestamp', 'unknown time')
        query = exchange.get('query', '')
        response_summary = exchange.get('response_summary', '')
        files_reviewed = exchange.get('files_reviewed', [])
        image_analyzed = exchange.get('image_analyzed', None)

        history_lines.append(f"\n### Exchange {i} ({timestamp})")
        history_lines.append(f"**Query:** {query[:500]}{'...' if len(query) > 500 else ''}")
        if files_reviewed:
            history_lines.append(f"**Files reviewed:** {', '.join(files_reviewed)}")
        if image_analyzed:
            history_lines.append(f"**Image analyzed:** {image_analyzed}")
        history_lines.append(f"**Response:** {response_summary[:500]}{'...' if len(response_summary) > 500 else ''}")

    history_lines.append("\n" + "‚îÄ" * 50)
    history_lines.append("## END OF HISTORY\n")

    formatted = "\n".join(history_lines)
    tokens = estimate_tokens(formatted)
    log_debug(f"Formatted history: {len(formatted)} chars, ~{tokens} tokens", debug)
    return (formatted, tokens)


def summarize_response(response: dict) -> str:
    """Create a brief summary of an Oracle response for history."""
    if "error" in response:
        return f"Error: {response['error']}"

    parts = []
    if "decision" in response:
        parts.append(f"Decision: {response['decision'][:200]}")
    if "steps" in response:
        parts.append(f"Steps: {len(response['steps'])} steps planned")
    if "risks" in response:
        parts.append(f"Risks: {len(response['risks'])} identified")

    return " | ".join(parts) if parts else "Response received"


def get_context_file(debug: bool = False) -> Tuple[Optional[str], int]:
    """Find and read the FULLAUTO_CONTEXT.md file (excluding appended fullauto instructions)."""
    current = Path.cwd()
    log_debug(f"Searching for FULLAUTO_CONTEXT.md starting from: {current}", debug)

    for i in range(10):  # Max 10 levels up
        context_file = current / "FULLAUTO_CONTEXT.md"
        log_debug(f"  Checking: {context_file}", debug)

        if context_file.exists():
            log_info(f"Found context file: {context_file}", debug)
            content = context_file.read_text()

            # Strip out the auto-appended fullauto instructions (Oracle doesn't need them)
            disclaimer_marker = "<!-- AUTO-GENERATED FULLAUTO INSTRUCTIONS -->"
            if disclaimer_marker in content:
                content = content[:content.find(disclaimer_marker)].rstrip()
                log_debug("Stripped appended fullauto instructions from context", debug)

            tokens = estimate_tokens(content)
            log_debug(f"Context file size: {len(content)} chars, ~{tokens} tokens", debug)
            return (content, tokens)

        parent = current.parent
        if parent == current:
            break
        current = parent

    log_debug("No FULLAUTO_CONTEXT.md found in directory tree", debug)
    return (None, 0)


def ask_oracle(
    query: str,
    mode: str = "plan",
    context_override: Optional[str] = None,
    thinking_level: str = "HIGH",
    debug: bool = False,
    no_history: bool = False,
    files: Optional[List[str]] = None,
    image_path: Optional[str] = None
) -> dict:
    """
    Ask the Gemini Oracle a question.

    Args:
        query: The question or task description
        mode: "plan" for new tasks, "validate" for checking progress
        context_override: Optional context to use instead of file
        thinking_level: LOW, MEDIUM, or HIGH
        debug: Enable debug output
        no_history: If True, don't use or save history
        files: Optional list of file specs to attach
        image_path: Optional path to an image to analyze

    Returns:
        Structured JSON response from Gemini
    """
    log_info(f"=== Oracle Ask Started ===", debug)
    log_debug(f"Mode: {mode}, Thinking: {thinking_level}", debug)
    log_debug(f"Query length: {len(query)} chars", debug)

    # Check for any valid authentication method
    api_key = get_gemini_api_key()
    vertex_key = os.environ.get("VERTEX_API_KEY")
    oauth_creds = get_oauth_credentials(debug)

    if not api_key and not vertex_key and not oauth_creds:
        log_error("No authentication available. Set VERTEX_API_KEY, GEMINI_API_KEY, GOOGLE_API_KEY, or run 'oracle login'")
        return {"error": "No authentication available. Set VERTEX_API_KEY, GEMINI_API_KEY, GOOGLE_API_KEY, or run 'oracle login'"}

    if api_key:
        log_debug(f"API key found: {api_key[:8]}...{api_key[-4:]}", debug)
    elif vertex_key:
        log_debug(f"Vertex API key found: {vertex_key[:8]}...{vertex_key[-4:]}", debug)
    else:
        log_debug("Using OAuth credentials", debug)

    # Track total tokens
    total_tokens = estimate_tokens(query)
    all_warnings = []

    # Process image if provided
    image_data = None
    image_mime_type = None
    image_analyzed = None
    if image_path:
        image_data, image_mime_type, error = read_image_as_base64(image_path, debug)
        if error:
            log_error(error)
            return {"error": error}
        image_analyzed = Path(image_path).name
        log_info(f"Image loaded: {image_analyzed}", debug)

    # Get project ID and history
    project_id = get_project_id(debug)
    history = [] if no_history else load_history(project_id, debug)
    history_section, history_tokens = format_history_for_prompt(history, debug)
    total_tokens += history_tokens

    # Process attached files
    files_section = ""
    files_reviewed = []
    if files:
        files_section, files_tokens, files_reviewed, file_warnings = format_files_for_prompt(files, debug)
        total_tokens += files_tokens
        all_warnings.extend(file_warnings)

    # Get project context
    if context_override:
        context = context_override
        context_tokens = estimate_tokens(context)
    else:
        context, context_tokens = get_context_file(debug)

    total_tokens += context_tokens

    if context:
        context_section = f"\n\n## PROJECT CONTEXT:\n{context}"
        log_debug(f"Using context: {len(context)} chars, ~{context_tokens} tokens", debug)
    else:
        context_section = "\n\n(No FULLAUTO_CONTEXT.md found - operating without project context)"
        log_debug("No context file - proceeding without project context", debug)

    # Check total token count
    log_info(f"Total estimated tokens: ~{total_tokens:,}", debug)

    if total_tokens >= TOKEN_WARNING_THRESHOLD:
        warning = f"‚ö†Ô∏è APPROACHING TOKEN LIMIT: ~{total_tokens:,} tokens (limit: 200k). Consider: 1) Clear oracle history: `oracle history --clear` 2) Send smaller file sections 3) Trim FULLAUTO_CONTEXT.md"
        all_warnings.append(warning)
        log_warning(warning)

    # Print all warnings
    for warning in all_warnings:
        print(warning, file=sys.stderr)

    # Select system prompt and schema based on mode
    if image_path and mode == "plan":
        # Use image analysis prompt when image is provided
        system_prompt = IMAGE_ANALYSIS_SYSTEM_PROMPT + "\n\n" + ORACLE_SYSTEM_PROMPT
    elif mode == "validate":
        system_prompt = VALIDATION_SYSTEM_PROMPT
        schema = VALIDATION_RESPONSE_SCHEMA
        log_debug("Using VALIDATION mode", debug)
    else:
        system_prompt = ORACLE_SYSTEM_PROMPT

    schema = VALIDATION_RESPONSE_SCHEMA if mode == "validate" else ORACLE_RESPONSE_SCHEMA
    log_debug(f"Using {'VALIDATION' if mode == 'validate' else 'PLAN'} mode", debug)

    # Build the full text prompt with all sections
    # Order: Query -> Files -> History -> Context
    full_text_prompt = f"{query}{files_section}{history_section}{context_section}"
    log_debug(f"Full prompt length: {len(full_text_prompt)} chars", debug)

    # Determine thinking level (Gemini 3 Pro uses "high"/"low" instead of token budget)
    thinking_level_value = "high" if thinking_level in ("HIGH", "MEDIUM") else "low"
    log_debug(f"Thinking level: {thinking_level_value}", debug)

    try:
        log_info("Creating Gemini client...", debug)
        client, auth_method = get_genai_client(api_key, debug)
        if client is None:
            return {"error": auth_method}
        log_info(f"Authenticated via {auth_method}", debug)

        # Build content parts
        parts = []

        # Add image first if provided
        if image_data and image_mime_type:
            parts.append(types.Part.from_bytes(data=image_data, mime_type=image_mime_type))
            log_debug("Added image to request", debug)

        # Add text prompt
        parts.append(types.Part.from_text(text=full_text_prompt))

        contents = [
            types.Content(role="user", parts=parts),
        ]

        log_debug("Building GenerateContentConfig...", debug)
        config = types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(thinking_level=thinking_level_value),
            response_mime_type="application/json",
            response_schema=schema,
            system_instruction=[types.Part.from_text(text=system_prompt)],
        )

        log_info("Calling Gemini API (this may take a moment)...", debug)
        response = client.models.generate_content(
            model="gemini-3-pro-preview",
            contents=contents,
            config=config,
        )

        log_debug(f"Response received", debug)

        # Check for response content
        if not response.text:
            log_error("Empty response from Gemini")
            return {"error": "Empty response from Gemini API"}

        log_debug(f"Response text length: {len(response.text)} chars", debug)
        log_debug(f"Raw response preview: {response.text[:200]}...", debug)

        # Parse the JSON response
        try:
            result = json.loads(response.text)
            log_info("Successfully parsed JSON response", debug)

            # Log response structure
            if debug:
                log_debug(f"Response keys: {list(result.keys())}", debug)
                if "steps" in result:
                    log_debug(f"Number of steps: {len(result['steps'])}", debug)
                if "risks" in result:
                    log_debug(f"Number of risks: {len(result['risks'])}", debug)

            # Save to history (unless disabled)
            # Note: File CONTENTS are not saved, only the file specs
            if not no_history:
                exchange = {
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
                    "query": query,
                    "files_reviewed": files_reviewed,  # Just the specs, not contents
                    "image_analyzed": image_analyzed,  # Just the filename
                    "response_summary": summarize_response(result),
                    "mode": mode
                }
                history.append(exchange)
                save_history(project_id, history, debug)

            return result

        except json.JSONDecodeError as e:
            log_error(f"Failed to parse JSON response: {e}")
            log_error(f"Raw response: {response.text}")
            return {"error": f"JSON parse error: {e}", "raw_response": response.text}

    except Exception as e:
        log_error(f"Gemini API call failed: {type(e).__name__}: {e}")
        import traceback
        log_debug(f"Full traceback:\n{traceback.format_exc()}", debug)
        return {"error": str(e), "error_type": type(e).__name__}


# =============================================================================
# VAST.AI AUTO-PROVISIONING
# =============================================================================

def vastai_api_request(method: str, endpoint: str, payload: dict = None, debug: bool = False) -> dict:
    """Make an authenticated request to Vast.ai API."""
    if requests is None:
        raise RuntimeError("requests library not installed. Run: pip install requests")

    api_key = os.environ.get("VAST_API_KEY")
    if not api_key:
        raise RuntimeError("VAST_API_KEY environment variable not set")

    url = f"{VASTAI_API_BASE}{endpoint}"
    headers = {"Authorization": f"Bearer {api_key}"}

    log_debug(f"Vast.ai API: {method} {endpoint}", debug)

    try:
        if method == "GET":
            resp = requests.get(url, headers=headers, params=payload, timeout=30)
        elif method == "POST":
            resp = requests.post(url, headers=headers, json=payload, timeout=30)
        elif method == "PUT":
            resp = requests.put(url, headers=headers, json=payload, timeout=30)
        elif method == "DELETE":
            resp = requests.delete(url, headers=headers, timeout=30)
        else:
            raise ValueError(f"Unknown method: {method}")

        resp.raise_for_status()
        return resp.json() if resp.text else {}

    except requests.exceptions.RequestException as e:
        log_error(f"Vast.ai API error: {e}")
        raise


def get_active_us_instance(debug: bool = False) -> Optional[Dict]:
    """Find an active US-based Vast.ai instance owned by user."""
    try:
        instances = vastai_api_request("GET", "/instances/", debug=debug)

        if not isinstance(instances, dict) or "instances" not in instances:
            log_debug(f"Unexpected instances response: {instances}", debug)
            return None

        for inst in instances.get("instances", []):
            status = inst.get("actual_status", "")
            geo = inst.get("geolocation", "")

            log_debug(f"Instance {inst.get('id')}: status={status}, geo={geo}", debug)

            # Check if running and in US
            if status == "running" and geo and ("US" in geo or "United States" in geo):
                # Extract SSH connection details
                ssh_host = inst.get("ssh_host", inst.get("public_ipaddr"))
                ssh_port = inst.get("ssh_port", 22)

                if ssh_host and ssh_port:
                    log_info(f"Found active US instance: {inst.get('id')} at {ssh_host}:{ssh_port}", debug)
                    return {
                        "id": inst.get("id"),
                        "host": ssh_host,
                        "port": ssh_port,
                        "user": "root",
                        "geo": geo,
                        "price": inst.get("dph_total", 0),
                    }

        log_debug("No active US instances found", debug)
        return None

    except Exception as e:
        log_error(f"Failed to check active instances: {e}")
        return None


def find_cheapest_us_offer(debug: bool = False) -> Optional[Dict]:
    """Find the cheapest available US-based Vast.ai offer."""
    if requests is None:
        raise RuntimeError("requests library not installed. Run: pip install requests")

    api_key = os.environ.get("VAST_API_KEY")
    if not api_key:
        raise RuntimeError("VAST_API_KEY environment variable not set")

    try:
        max_price = VASTAI_AUTO_PROVISION['max_price_per_hour']

        # Vast.ai uses POST with JSON body for search
        payload = {
            "verified": {"eq": True},
            "external": {"eq": False},
            "rentable": {"eq": True},
            "rented": {"eq": False},
            "reliability": {"gte": 0.95},
            "num_gpus": {"gte": 1},
            "inet_up": {"gte": 100},
            "geolocation": {"in": ["US"]},
            "dph_total": {"lte": max_price},
            "order": [["dph_total", "asc"]],
            "type": "on-demand",
            "limit": 20,
            "allocated_storage": VASTAI_AUTO_PROVISION['disk_gb'],
        }

        url = f"{VASTAI_API_BASE}/bundles/?api_key={api_key}"
        log_info("Searching for cheapest US offers...", debug)
        log_debug(f"POST {url}", debug)

        resp = requests.post(url, json=payload, timeout=30)
        resp.raise_for_status()
        data = resp.json()

        offers = data.get("offers", [])
        log_debug(f"Found {len(offers)} US offers within ${max_price}/hr", debug)

        if not offers:
            log_warning(f"No US offers available within ${max_price}/hr limit")
            return None

        # Already sorted by price, take the first one
        cheapest = offers[0]

        log_info(f"Cheapest US offer: ${cheapest.get('dph_total', 0):.3f}/hr, GPU: {cheapest.get('gpu_name')}, Geo: {cheapest.get('geolocation')}", debug)

        return cheapest

    except Exception as e:
        log_error(f"Failed to search offers: {e}")
        return None


def create_vastai_instance(offer_id: int, debug: bool = False) -> Optional[int]:
    """Create a new Vast.ai instance from an offer."""
    try:
        payload = {
            "client_id": "me",
            "image": VASTAI_AUTO_PROVISION['docker_image'],
            "disk": VASTAI_AUTO_PROVISION['disk_gb'],
            "label": "oracle-image-gen",
            "onstart": "pip install -q google-genai",  # Pre-install dependency
        }

        log_info(f"Creating instance from offer {offer_id}...", debug)
        resp = vastai_api_request("PUT", f"/asks/{offer_id}/", payload=payload, debug=debug)

        instance_id = resp.get("new_contract")
        if instance_id:
            log_info(f"Created instance: {instance_id}", debug)
            return instance_id
        else:
            log_error(f"No instance ID in response: {resp}")
            return None

    except Exception as e:
        log_error(f"Failed to create instance: {e}")
        return None


def wait_for_instance_ready(instance_id: int, debug: bool = False) -> Optional[Dict]:
    """Wait for an instance to be running and SSH-ready."""
    if requests is None:
        raise RuntimeError("requests library not installed")

    api_key = os.environ.get("VAST_API_KEY")
    if not api_key:
        raise RuntimeError("VAST_API_KEY not set")

    startup_timeout = VASTAI_AUTO_PROVISION['startup_timeout']
    ssh_timeout = VASTAI_AUTO_PROVISION['ssh_timeout']

    log_info(f"Waiting for instance {instance_id} to be ready (max {startup_timeout}s)...", debug)

    start_time = time.time()
    ssh_host = None
    ssh_port = None

    # Initial wait - instances typically take 10-20s to start
    print("üîÆ Waiting 10s for instance to initialize...", file=sys.stderr)
    time.sleep(10)

    # Phase 1: Wait for instance to be running (check every 3s for faster detection)
    while time.time() - start_time < startup_timeout:
        try:
            # Use correct API format with api_key in URL
            url = f"{VASTAI_API_BASE}/instances/{instance_id}/?api_key={api_key}&owner=me"
            resp = requests.get(url, timeout=15)
            resp.raise_for_status()
            data = resp.json()

            # API wraps response in "instances" key
            if "instances" in data:
                data = data["instances"]

            status = data.get("actual_status", "")
            ssh_host = data.get("ssh_host")
            ssh_port = data.get("ssh_port")

            log_debug(f"Instance status: {status}, ssh: {ssh_host}:{ssh_port}", debug)

            if status == "running" and ssh_host and ssh_port:
                log_info(f"Instance running at {ssh_host}:{ssh_port}", debug)
                break
            elif status in ["exited", "error", "failed"]:
                log_error(f"Instance failed with status: {status}")
                return None

        except Exception as e:
            log_debug(f"Status check failed: {e}", debug)

        time.sleep(3)  # Check every 3s for faster detection
    else:
        log_error(f"Instance startup timed out after {startup_timeout}s")
        return None

    # Phase 2: Wait for SSH to be ready (check every 2s with fast timeout)
    log_info("Waiting for SSH to be ready...", debug)
    ssh_start = time.time()

    while time.time() - ssh_start < ssh_timeout:
        try:
            result = subprocess.run(
                [
                    "ssh", "-p", str(ssh_port),
                    "-o", "StrictHostKeyChecking=no",
                    "-o", "ConnectTimeout=3",
                    "-o", "BatchMode=yes",
                    f"root@{ssh_host}",
                    "echo 'SSH_READY'"
                ],
                capture_output=True,
                text=True,
                timeout=5
            )

            if result.returncode == 0 and "SSH_READY" in result.stdout:
                log_info("SSH is ready!", debug)
                return {
                    "id": instance_id,
                    "host": ssh_host,
                    "port": ssh_port,
                    "user": "root",
                }

        except Exception as e:
            log_debug(f"SSH check failed: {e}", debug)

        time.sleep(2)  # Check every 2s

    log_error(f"SSH not ready after {ssh_timeout}s")
    return None


def destroy_vastai_instance(instance_id: int, debug: bool = False) -> bool:
    """Destroy a Vast.ai instance."""
    if requests is None:
        log_error("requests library not installed")
        return False

    api_key = os.environ.get("VAST_API_KEY")
    if not api_key:
        log_error("VAST_API_KEY not set")
        return False

    try:
        log_info(f"Destroying instance {instance_id}...", debug)
        url = f"{VASTAI_API_BASE}/instances/{instance_id}/?api_key={api_key}"
        resp = requests.delete(url, json={}, timeout=30)
        resp.raise_for_status()
        log_info(f"Instance {instance_id} destroyed", debug)
        return True
    except Exception as e:
        log_error(f"Failed to destroy instance {instance_id}: {e}")
        return False


def auto_provision_us_instance(debug: bool = False) -> Tuple[Optional[Dict], bool]:
    """
    Auto-provision a US-based Vast.ai instance.

    Returns:
        Tuple of (instance_info, was_created)
        - instance_info: Dict with host, port, user, id or None if failed
        - was_created: True if we created this instance (need to destroy later)
    """
    # First, check for existing active US instance
    log_info("Checking for existing US instances...", debug)
    existing = get_active_us_instance(debug)

    if existing:
        print(f"üîÆ Reusing existing US instance: {existing['host']}:{existing['port']} (${existing.get('price', 0):.3f}/hr)", file=sys.stderr)
        return (existing, False)  # Not created by us, don't destroy

    # No existing instance, need to provision one
    print("üîÆ No US instance found. Auto-provisioning cheapest available...", file=sys.stderr)

    offer = find_cheapest_us_offer(debug)
    if not offer:
        print("üîÆ ERROR: No suitable US instances available", file=sys.stderr)
        return (None, False)

    print(f"üîÆ Found offer: ${offer.get('dph_total', 0):.3f}/hr, {offer.get('gpu_name')}", file=sys.stderr)

    instance_id = create_vastai_instance(offer["id"], debug)
    if not instance_id:
        return (None, False)

    print(f"üîÆ Instance {instance_id} created. Waiting for startup...", file=sys.stderr)

    instance_info = wait_for_instance_ready(instance_id, debug)
    if not instance_info:
        # Failed to start, destroy the failed instance
        destroy_vastai_instance(instance_id, debug)
        return (None, False)

    print(f"üîÆ Instance ready: {instance_info['host']}:{instance_info['port']}", file=sys.stderr)
    return (instance_info, True)  # We created it, need to destroy after


def generate_remote_image_script(prompt: str, api_key: str, output_filename: str) -> str:
    """
    Generate a Python script to run on the remote server for image generation.

    The script:
    1. Installs google-generativeai if needed
    2. Generates the image using Gemini
    3. Saves to /tmp and prints the path
    """
    # Escape the prompt for shell safety
    escaped_prompt = prompt.replace("'", "'\\''").replace('"', '\\"')

    script = f'''
import subprocess
import sys

# Install dependencies quietly (google-genai is the correct package name)
result = subprocess.run([sys.executable, "-m", "pip", "install", "-q", "google-genai", "Pillow"],
                        capture_output=True, text=True)
if result.returncode != 0:
    print(f"ERROR: pip install failed: {{result.stderr}}", file=sys.stderr)
    sys.exit(1)

import os
from google import genai
from google.genai import types

api_key = os.environ.get("GEMINI_API_KEY")
if not api_key:
    print("ERROR: No API key", file=sys.stderr)
    sys.exit(1)

client = genai.Client(api_key=api_key)

prompt = """{escaped_prompt}"""

try:
    config = types.GenerateContentConfig(
        response_modalities=["IMAGE", "TEXT"],
        image_config=types.ImageConfig(image_size="2K"),
    )

    for chunk in client.models.generate_content_stream(
        model="gemini-3-pro-image-preview",
        contents=[types.Content(role="user", parts=[types.Part.from_text(text=prompt)])],
        config=config,
    ):
        if chunk.candidates is None or chunk.candidates[0].content is None or chunk.candidates[0].content.parts is None:
            continue
        for part in chunk.candidates[0].content.parts:
            if hasattr(part, "inline_data") and part.inline_data and part.inline_data.data:
                output_path = "/tmp/{output_filename}"
                with open(output_path, "wb") as f:
                    f.write(part.inline_data.data)
                print(output_path)  # This is the only stdout - the path
                sys.exit(0)

    print("ERROR: No image in response", file=sys.stderr)
    sys.exit(1)

except Exception as e:
    print(f"ERROR: {{e}}", file=sys.stderr)
    sys.exit(1)
'''
    return script


def run_on_vastai_instance(
    instance: Dict,
    script: str,
    api_key: str,
    local_output_path: Path,
    debug: bool = False
) -> Tuple[Optional[str], Optional[str]]:
    """
    Run an image generation script on a Vast.ai instance.

    Args:
        instance: Dict with host, port, user
        script: Python script to execute
        api_key: Gemini API key to pass
        local_output_path: Where to save the result locally
        debug: Enable debug output

    Returns:
        Tuple of (saved_path, error_message)
    """
    host = instance['host']
    port = instance['port']
    user = instance.get('user', 'root')
    timeout = VASTAI_CONFIG['timeout']

    log_debug(f"Running on {user}@{host}:{port}", debug)

    try:
        # Execute script on remote server via stdin
        ssh_cmd = [
            "ssh",
            "-p", str(port),
            "-o", "StrictHostKeyChecking=no",
            "-o", "ConnectTimeout=10",
            f"{user}@{host}",
            f'GEMINI_API_KEY="{api_key}" python3 -'
        ]

        log_info("Executing image generation on Vast.ai...", debug)

        result = subprocess.run(
            ssh_cmd,
            input=script,
            capture_output=True,
            text=True,
            timeout=timeout
        )

        log_debug(f"SSH exit code: {result.returncode}", debug)
        if result.stderr:
            log_debug(f"SSH stderr: {result.stderr}", debug)
        if result.stdout:
            log_debug(f"SSH stdout: {result.stdout}", debug)

        if result.returncode != 0:
            error_msg = result.stderr.strip() if result.stderr else "Unknown SSH error"
            # Check for geo-restriction on remote too
            if "not available in your country" in error_msg.lower():
                return (None, "Remote server also geo-restricted. Need US-based instance.")
            return (None, f"Remote execution failed: {error_msg}")

        # Get the remote path from stdout
        remote_output = result.stdout.strip()
        if not remote_output or not remote_output.startswith("/tmp/"):
            return (None, f"Invalid remote output path: {remote_output}")

        log_info(f"Image generated on remote: {remote_output}", debug)

        # SCP the file back
        log_info("Transferring image from Vast.ai...", debug)

        scp_cmd = [
            "scp",
            "-P", str(port),
            "-o", "StrictHostKeyChecking=no",
            f"{user}@{host}:{remote_output}",
            str(local_output_path)
        ]

        scp_result = subprocess.run(scp_cmd, capture_output=True, text=True, timeout=30)

        if scp_result.returncode != 0:
            return (None, f"SCP failed: {scp_result.stderr}")

        log_info(f"Image transferred to: {local_output_path}", debug)

        # Cleanup remote file
        cleanup_cmd = [
            "ssh",
            "-p", str(port),
            "-o", "StrictHostKeyChecking=no",
            f"{user}@{host}",
            f"rm -f {remote_output}"
        ]
        subprocess.run(cleanup_cmd, capture_output=True, timeout=10)
        log_debug("Remote temp file cleaned up", debug)

        return (str(local_output_path), None)

    except subprocess.TimeoutExpired:
        return (None, f"Remote operation timed out after {timeout}s")
    except FileNotFoundError:
        return (None, "SSH/SCP not found. Ensure OpenSSH is installed.")
    except Exception as e:
        log_error(f"Remote execution failed: {type(e).__name__}: {e}")
        return (None, str(e))


def imagine_via_vastai(
    prompt: str,
    api_key: str,
    local_output_path: Path,
    debug: bool = False
) -> Tuple[Optional[str], Optional[str]]:
    """
    Generate an image via Vast.ai server to bypass geo-restrictions.

    This function will:
    1. Check for existing active US-based instance
    2. If none, auto-provision a new US instance
    3. Run the image generation
    4. Destroy the instance if we created it

    Args:
        prompt: Image generation prompt
        api_key: Gemini API key
        local_output_path: Where to save the image locally
        debug: Enable debug output

    Returns:
        Tuple of (saved_path, error_message)
    """
    log_info("=== Falling back to Vast.ai for image generation ===", debug)

    # Check if Vast.ai API key is available for auto-provisioning
    vast_api_key = os.environ.get("VAST_API_KEY")

    instance = None
    instance_created = False

    if vast_api_key and VASTAI_AUTO_PROVISION['enabled']:
        # Try auto-provisioning
        print("üîÆ Checking for US-based Vast.ai instances...", file=sys.stderr)
        try:
            instance, instance_created = auto_provision_us_instance(debug)
        except Exception as e:
            log_error(f"Auto-provision failed: {e}")
            print(f"üîÆ Auto-provision failed: {e}", file=sys.stderr)

    if not instance:
        # Fallback to configured instance (may not be US-based)
        log_info("Using configured Vast.ai instance (may not be US-based)", debug)
        print("üîÆ No VAST_API_KEY set or auto-provision failed. Using configured instance.", file=sys.stderr)
        instance = {
            'host': VASTAI_CONFIG['host'],
            'port': VASTAI_CONFIG['port'],
            'user': VASTAI_CONFIG['user'],
        }

    # Generate the script
    remote_filename = f"oracle_img_{uuid.uuid4().hex[:8]}.png"
    script = generate_remote_image_script(prompt, api_key, remote_filename)

    try:
        # Run on the instance
        result_path, error = run_on_vastai_instance(
            instance, script, api_key, local_output_path, debug
        )

        return (result_path, error)

    finally:
        # Cleanup: destroy instance if we created it
        if instance_created and instance.get('id'):
            print(f"üîÆ Destroying temporary instance {instance['id']}...", file=sys.stderr)
            destroy_vastai_instance(instance['id'], debug)
            print("üîÆ Instance destroyed. Cost saved!", file=sys.stderr)


def imagine(
    prompt: str,
    output_path: Optional[str] = None,
    reference_image: Optional[str] = None,
    debug: bool = False
) -> Tuple[Optional[str], Optional[str]]:
    """
    Generate an image using the Oracle.

    Args:
        prompt: Description of the image to generate
        output_path: Optional path to save the image (default: auto-generated)
        reference_image: Optional reference image to base generation on
        debug: Enable debug output

    Returns:
        Tuple of (saved_path, error_message)
    """
    log_info("=== Oracle Imagine Started ===", debug)
    log_debug(f"Prompt: {prompt[:100]}...", debug)

    # Check for any valid authentication method
    api_key = get_gemini_api_key()
    vertex_key = os.environ.get("VERTEX_API_KEY")
    oauth_creds = get_oauth_credentials(debug)

    if not api_key and not vertex_key and not oauth_creds:
        return (None, "No authentication available. Set VERTEX_API_KEY, GEMINI_API_KEY, GOOGLE_API_KEY, or run 'oracle login'")

    # Pre-compute output path for potential Vast.ai fallback
    if output_path:
        save_path = Path(output_path)
        if not save_path.suffix:
            save_path = save_path.with_suffix('.png')
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"oracle_imagine_{timestamp}.png"
        save_path = IMAGE_OUTPUT_DIR / filename

    # Ensure parent directory exists
    save_path.parent.mkdir(parents=True, exist_ok=True)

    # Process reference image if provided
    ref_image_data = None
    ref_image_mime_type = None
    if reference_image:
        ref_image_data, ref_image_mime_type, error = read_image_as_base64(reference_image, debug)
        if error:
            return (None, error)
        log_info(f"Reference image loaded: {reference_image}", debug)

    try:
        log_info("Creating Gemini client...", debug)
        client, auth_method = get_genai_client(api_key, debug)
        if client is None:
            return (None, auth_method)
        log_info(f"Authenticated via {auth_method}", debug)

        # Build content parts
        parts = []

        # Add reference image first if provided
        if ref_image_data and ref_image_mime_type:
            parts.append(types.Part.from_bytes(data=ref_image_data, mime_type=ref_image_mime_type))
            log_debug("Added reference image to request", debug)

        # Add text prompt
        parts.append(types.Part.from_text(text=prompt))

        contents = [
            types.Content(role="user", parts=parts),
        ]

        log_debug("Building GenerateContentConfig for image generation...", debug)
        config = types.GenerateContentConfig(
            response_modalities=["IMAGE", "TEXT"],
            image_config=types.ImageConfig(image_size="2K"),
        )

        log_info("Calling Gemini API for image generation (streaming)...", debug)

        # Use streaming for gemini-3-pro-image-preview
        for chunk in client.models.generate_content_stream(
            model="gemini-3-pro-image-preview",
            contents=contents,
            config=config,
        ):
            if (
                chunk.candidates is None
                or chunk.candidates[0].content is None
                or chunk.candidates[0].content.parts is None
            ):
                continue

            for part in chunk.candidates[0].content.parts:
                if hasattr(part, 'inline_data') and part.inline_data and part.inline_data.data:
                    inline_data = part.inline_data
                    image_bytes = inline_data.data
                    mime_type = inline_data.mime_type

                    # Determine file extension from response
                    ext = mimetypes.guess_extension(mime_type) or '.png'
                    if ext == '.jpe':
                        ext = '.jpg'

                    # Update save_path with correct extension if different
                    if save_path.suffix != ext:
                        save_path = save_path.with_suffix(ext)

                    # Save the image
                    with open(save_path, 'wb') as f:
                        f.write(image_bytes)

                    log_info(f"Image saved to: {save_path}", debug)
                    return (str(save_path), None)

                elif hasattr(part, 'text') and part.text:
                    # Log any text response (might contain additional info)
                    log_debug(f"Text in response: {part.text[:200]}", debug)

        return (None, "No image data in response")

    except Exception as e:
        error_str = str(e).lower()
        log_error(f"Image generation failed: {type(e).__name__}: {e}")

        # Check for geo-restriction error
        if "not available in your country" in error_str or "geo" in error_str:
            log_warning("Geo-restriction detected. Attempting Vast.ai fallback...")
            print("üîÆ Geo-restricted locally. Routing through Vast.ai...", file=sys.stderr)

            # Note: reference image not supported via Vast.ai fallback (too complex)
            if reference_image:
                log_warning("Reference image not supported via Vast.ai fallback")
                print("üîÆ WARNING: Reference image ignored for remote generation", file=sys.stderr)

            return imagine_via_vastai(prompt, api_key, save_path, debug)

        import traceback
        log_debug(f"Full traceback:\n{traceback.format_exc()}", debug)
        return (None, str(e))


def quick_ask(query: str, debug: bool = False, no_history: bool = False) -> str:
    """Quick question without structured output - just get a text answer."""
    log_info("=== Quick Ask Started ===", debug)

    # Check for any valid authentication method
    api_key = get_gemini_api_key()
    vertex_key = os.environ.get("VERTEX_API_KEY")
    oauth_creds = get_oauth_credentials(debug)

    if not api_key and not vertex_key and not oauth_creds:
        log_error("No authentication available. Set VERTEX_API_KEY, GEMINI_API_KEY, GOOGLE_API_KEY, or run 'oracle login'")
        return "ERROR: No authentication available. Set VERTEX_API_KEY, GEMINI_API_KEY, GOOGLE_API_KEY, or run 'oracle login'"

    # Get project ID and history
    project_id = get_project_id(debug)
    history = [] if no_history else load_history(project_id, debug)
    history_section, _ = format_history_for_prompt(history, debug)

    context, _ = get_context_file(debug)
    context_section = f"\n\nContext:\n{context}" if context else ""

    # Build full prompt with history
    full_prompt = f"{query}{history_section}{context_section}"

    try:
        log_info("Creating Gemini client...", debug)
        client, auth_method = get_genai_client(api_key, debug)
        if client is None:
            return f"ERROR: {auth_method}"
        log_info(f"Authenticated via {auth_method}", debug)

        log_info("Calling Gemini API...", debug)
        response = client.models.generate_content(
            model="gemini-3-pro-preview",
            contents=full_prompt,
            config=types.GenerateContentConfig(
                thinking_config=types.ThinkingConfig(thinking_level="high"),
                system_instruction="You are a helpful AI assistant. Be concise and direct. You have access to conversation history - use it for context.",
            ),
        )

        log_info("Response received", debug)

        # Save to history
        if not no_history:
            exchange = {
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
                "query": query,
                "response_summary": response.text[:200] + "..." if len(response.text) > 200 else response.text,
                "mode": "quick"
            }
            history.append(exchange)
            save_history(project_id, history, debug)

        return response.text

    except Exception as e:
        log_error(f"Quick ask failed: {type(e).__name__}: {e}")
        return f"ERROR: {e}"


def main():
    parser = argparse.ArgumentParser(
        description="Gemini Oracle - AI Orchestration for Claude Code",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  oracle ask "How should I implement a caching layer?"
  oracle ask --mode=validate "I've implemented the database schema"
  oracle ask --files src/main.py "Review this file"
  oracle ask --files "src/main.py:1-50,100-110" "Check these sections"
  oracle ask --image screenshot.png "What's wrong with this UI?"
  oracle imagine "Minimalist logo for fintech startup" --output logo.png
  oracle imagine --ref current.png "Make this more modern"
  oracle quick "What's the best Python HTTP library?"
  oracle history                    # View conversation history
  oracle history --clear            # Clear history

File specifications:
  src/main.py              # Whole file
  src/main.py:10-50        # Lines 10-50
  src/main.py:1-50,100-110 # Lines 1-50 and 100-110

Log files: ~/.oracle/logs/
History files: ~/.oracle/history/
Generated images: ~/.oracle/images/
        """
    )

    subparsers = parser.add_subparsers(dest="command", help="Command to run")

    # ask command
    ask_parser = subparsers.add_parser("ask", help="Ask the oracle for guidance")
    ask_parser.add_argument("query", help="Your question or task description")
    ask_parser.add_argument(
        "--mode",
        choices=["plan", "validate"],
        default="plan",
        help="Mode: 'plan' for new tasks, 'validate' for checking progress"
    )
    ask_parser.add_argument(
        "--thinking",
        choices=["LOW", "MEDIUM", "HIGH"],
        default="HIGH",
        help="Thinking depth level"
    )
    ask_parser.add_argument(
        "--context",
        help="Override context (instead of reading from file)"
    )
    ask_parser.add_argument(
        "--files",
        help="Comma-separated list of files to attach (supports line ranges: file.py:1-50,100-110)"
    )
    ask_parser.add_argument(
        "--image",
        help="Path to an image file to analyze (screenshot, diagram, mockup, etc.)"
    )
    ask_parser.add_argument(
        "--pretty",
        action="store_true",
        help="Pretty-print JSON output"
    )
    ask_parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug output to stderr"
    )
    ask_parser.add_argument(
        "--no-history",
        action="store_true",
        help="Don't use or save conversation history"
    )

    # imagine command
    imagine_parser = subparsers.add_parser("imagine", help="Generate an image")
    imagine_parser.add_argument("prompt", help="Description of the image to generate")
    imagine_parser.add_argument(
        "--output", "-o",
        help="Output file path (default: auto-generated in ~/.oracle/images/)"
    )
    imagine_parser.add_argument(
        "--ref",
        help="Reference image to base generation on (for modifications/iterations)"
    )
    imagine_parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug output"
    )

    # quick command
    quick_parser = subparsers.add_parser("quick", help="Quick question, text response")
    quick_parser.add_argument("query", help="Your question")
    quick_parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug output"
    )
    quick_parser.add_argument(
        "--no-history",
        action="store_true",
        help="Don't use or save conversation history"
    )

    # history command
    history_parser = subparsers.add_parser("history", help="View or manage conversation history")
    history_parser.add_argument(
        "--clear",
        action="store_true",
        help="Clear history for current project"
    )
    history_parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug output"
    )

    # info command
    info_parser = subparsers.add_parser("info", help="Show oracle status and context")

    # logs command
    logs_parser = subparsers.add_parser("logs", help="Show recent logs")
    logs_parser.add_argument(
        "-n", "--lines",
        type=int,
        default=50,
        help="Number of lines to show"
    )

    # context command - view FULLAUTO_CONTEXT.md
    context_parser = subparsers.add_parser("context", help="Show FULLAUTO_CONTEXT.md")

    # login command - OAuth login with Google account
    login_parser = subparsers.add_parser("login", help="Login with Google account (uses your subscription)")

    # logout command - remove OAuth credentials
    logout_parser = subparsers.add_parser("logout", help="Logout and remove saved credentials")

    args = parser.parse_args()

    # Check FULLAUTO_CONTEXT.md health (not gitignored, is tracked)
    check_fullauto_context_health()

    # Auto-prepend recovery header to FULLAUTO_CONTEXT.md if it exists without one
    ensure_fullauto_header()

    if args.command == "ask":
        # Parse files argument
        files = None
        if args.files:
            # Split by comma, but be careful with line ranges that also use commas
            # We split on commas that are followed by a path (not a number)
            files = []
            current_spec = ""
            for part in args.files.split(','):
                part = part.strip()
                # Check if this looks like a continuation of line ranges (just numbers with dash)
                if current_spec and re.match(r'^\d+-?\d*$', part):
                    current_spec += ',' + part
                else:
                    if current_spec:
                        files.append(current_spec)
                    current_spec = part
            if current_spec:
                files.append(current_spec)

        result = ask_oracle(
            query=args.query,
            mode=args.mode,
            context_override=args.context,
            thinking_level=args.thinking,
            debug=args.debug,
            no_history=args.no_history,
            files=files,
            image_path=args.image
        )
        if args.pretty:
            print(json.dumps(result, indent=2))
        else:
            print(json.dumps(result))

    elif args.command == "imagine":
        saved_path, error = imagine(
            prompt=args.prompt,
            output_path=args.output,
            reference_image=args.ref,
            debug=args.debug
        )

        if error:
            print(f"üîÆ ERROR: {error}", file=sys.stderr)
            sys.exit(1)
        else:
            print(f"üîÆ Image generated: {saved_path}")

    elif args.command == "quick":
        print(quick_ask(args.query, debug=args.debug, no_history=args.no_history))

    elif args.command == "history":
        project_id = get_project_id(args.debug)

        if args.clear:
            if clear_history(project_id, args.debug):
                print(f"‚úì History cleared for project: {project_id}")
            else:
                print(f"‚úó Failed to clear history")
        else:
            history = load_history(project_id, args.debug)
            if not history:
                print(f"No conversation history for project: {project_id}")
            else:
                print(f"=== Conversation History ({len(history)} exchanges) ===")
                print(f"Project: {project_id}")
                print("‚îÄ" * 50)
                for i, exchange in enumerate(history, 1):
                    print(f"\n[{i}] {exchange.get('timestamp', 'unknown')} ({exchange.get('mode', 'unknown')})")
                    print(f"    Q: {exchange.get('query', '')[:100]}...")
                    files_rev = exchange.get('files_reviewed', [])
                    if files_rev:
                        print(f"    Files: {', '.join(files_rev)}")
                    img = exchange.get('image_analyzed')
                    if img:
                        print(f"    Image: {img}")
                    print(f"    A: {exchange.get('response_summary', '')[:100]}...")
                print("\n" + "‚îÄ" * 50)
                print(f"History file: {get_history_file(project_id)}")

    elif args.command == "info":
        project_id = get_project_id()
        context, context_tokens = get_context_file()
        api_key = get_gemini_api_key()
        history = load_history(project_id)

        print("=== Gemini Oracle Status ===")
        vertex_key = os.environ.get("VERTEX_API_KEY")
        print(f"Vertex AI Key: {'SET' if vertex_key else 'NOT SET'}")
        print(f"API Key: {'SET' if api_key else 'NOT SET'}")
        print(f"Project ID: {project_id}")
        print(f"Context File: {'FOUND' if context else 'NOT FOUND'}")
        if context:
            print(f"Context Tokens: ~{context_tokens:,}")
        print(f"History: {len(history)} exchanges (max {MAX_HISTORY_EXCHANGES})")
        print(f"Log Directory: {LOG_DIR}")
        print(f"History Directory: {HISTORY_DIR}")
        print(f"Image Output: {IMAGE_OUTPUT_DIR}")

        if context:
            lines = context.split('\n')
            print(f"Context Size: {len(context)} chars, {len(lines)} lines")
            print(f"First line: {lines[0][:80]}...")

        # Check OAuth status
        oauth_creds = get_oauth_credentials(debug=False)
        if oauth_creds:
            print(f"OAuth Status: LOGGED IN (Google account)")
        elif OAUTH_TOKEN_FILE.exists():
            print(f"OAuth Status: TOKEN EXPIRED (run 'oracle login')")
        else:
            print(f"OAuth Status: NOT LOGGED IN")

        # Check if google-genai is properly configured
        try:
            client, auth_method = get_genai_client(api_key, debug=False)
            print(f"Gemini Client: {'OK via ' + auth_method if client else 'NOT CONFIGURED'}")
        except Exception as e:
            print(f"Gemini Client: ERROR - {e}")

    elif args.command == "logs":
        if LOG_FILE.exists():
            with open(LOG_FILE) as f:
                lines = f.readlines()
                for line in lines[-args.lines:]:
                    print(line, end='')
        else:
            print("No logs found for today")

    elif args.command == "context":
        context_file = Path.cwd() / "FULLAUTO_CONTEXT.md"
        if context_file.exists():
            print(context_file.read_text())
        else:
            print(f"No FULLAUTO_CONTEXT.md found in {Path.cwd()}")

    elif args.command == "login":
        oauth_login(debug=False)

    elif args.command == "logout":
        oauth_logout()

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
